{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for CS-165A  Coding Project 1: Classifier Agent\n",
    "\n",
    "### **Name:** David Jr Sim\n",
    "### **PERM \\#:** 5416763\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of Sources and Collaboration:\n",
    "\n",
    "### **Collaboration:** None\n",
    "\n",
    "### **Sources:** \n",
    "- Wikipedia, “Cross-entropy,” Wikipedia, https://en.wikipedia.org/wiki/Cross-entropy (accessed Jan. 22, 2024). \n",
    "- D. Shah, “Cross entropy loss: Intro, applications, code,” V7, https://www.v7labs.com/blog/cross-entropy-loss-guide (accessed Jan. 22, 2024). \n",
    "- Python3, “Collections - container datatypes,” Python documentation, https://docs.python.org/3/library/collections.html#collections.Counter (accessed Jan. 22, 2024). \n",
    "- Python3, “5. Data Structures,” Python documentation, https://docs.python.org/3/tutorial/datastructures.html (accessed Jan. 22, 2024). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Gradient Calculations\n",
    "\n",
    "The loss function to use is the cross-entropy loss, averaged over data points. We calculate the gradient with respect to the weights as that is what we want to change. The input (x) and label(y) are constants, so we can ignore them when calculating the gradient. The gradient is calculated as follows:\n",
    "\n",
    "$\n",
    "L(w) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{l}(w, (x_i, y_i)) \\\\\n",
    "\\mathbb{l}(w, (x_i, y_i)) = -(\\log \\hat{p}_w(x)y+\\log(1-\\hat{p}_w(x))(1-y)) \\\\\n",
    "\\hat{p}_w(x) = \\frac{e^{-w^Tx}}{1+e^{-w^Tx}} \\\\\n",
    "$\n",
    "<br/><br/><br/><br/>\n",
    "$\n",
    "\\nabla L(w) = \\nabla \\frac{1}{n} \\sum_{i=1}^n \\mathbb{l}(w, (x_i, y_i)) \\\\\n",
    "\\hspace{3.35em} = \\frac{1}{n} \\sum_{i=1}^n \\nabla \\mathbb{l}(w, (x_i, y_i)) \\\\\n",
    "$ \n",
    "<br/><br/><br/>\n",
    "$\n",
    "\\nabla \\mathbb{l}(w, (x_i, y_i)) = \\frac{\\partial}{\\partial w}[-(\\ln(\\frac{e^{-w^Tx}}{1+e^{-w^Tx}})y+\\ln(1-\\frac{e^{-w^Tx}}{1+e^{-w^Tx}})(1-y))] \\\\\n",
    "\\nabla \\mathbb{l}(w, (x_i, y_i)) = \\frac{\\partial}{\\partial w}[w^Txy+y\\ln (1+e^{-w^Tx})-\\ln (1-\\frac{e^{-w^Tx}}{1+e^{-w^Tx}})+y\\ln (1-\\frac{e^{-w^Tx}}{1+e^{-w^Tx}})] \\\\\n",
    "\\nabla \\mathbb{l}(w, (x_i, y_i)) = \\frac{\\partial}{\\partial w}(w^Txy)+\\frac{\\partial}{\\partial w}(y\\ln (1+e^{-w^Tx}))-\\frac{\\partial}{\\partial w}(\\ln (1-\\frac{e^{-w^Tx}}{1+e^{-w^Tx}}))+\\frac{\\partial}{\\partial w}(y\\ln (1-\\frac{e^{-w^Tx}}{1+e^{-w^Tx}}))\n",
    "$\n",
    "<br/><br/><br/>\n",
    "$\n",
    "\\frac{\\partial}{\\partial w}(w^Txy) = xy \\\\\n",
    "\\frac{\\partial}{\\partial w}(y\\ln (1+e^{-w^Tx})) = -\\frac{xye^{-w^Tx}}{1+e^{-w^Tx}} \\\\\n",
    "\\frac{\\partial}{\\partial w}(\\ln (1-\\frac{e^{-w^Tx}}{1+e^{-w^Tx}})) = \\frac{xe^{-w^Tx}}{1+e^{-w^Tx}} \\\\\n",
    "\\frac{\\partial}{\\partial w}(y\\ln (1-\\frac{e^{-w^Tx}}{1+e^{-w^Tx}})) = \\frac{xye^{-w^Tx}}{1+e^{-w^Tx}}\n",
    "$\n",
    "<br/><br/><br/>\n",
    "$\n",
    "\\nabla \\mathbb{l}(w, (x_i, y_i)) = xy-\\frac{xye^{-w^Tx}}{1+e^{-w^Tx}}-\\frac{xe^{-w^Tx}}{1+e^{-w^Tx}}+\\frac{xye^{-w^Tx}}{1+e^{-w^Tx}} \\\\\n",
    "\\nabla \\mathbb{l}(w, (x_i, y_i)) = xy-\\frac{xe^{-w^Tx}}{1+e^{-w^Tx}}\n",
    "$\n",
    "<br/><br/><br/>\n",
    "$\n",
    "\\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^n [x_iy_i-x_i(\\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}})] \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "![Error and gradient descent implemention data](./Images/results_format.png)\n",
    "\n",
    "In the end, the graphs show that both methods have reached about the same error rate. However, the biggest difference is time. While SGD was a lot less inconsistent with its progress through each iteration, it took a lot less time to reach the same error rate as vanilla GD. On the other hand, vanilla was a lot more consistent with its progress towards the low error rate, however it too much longer to reach it. Based on the graph, SGD took about 2 minutes to reach its final error rate while vanilla GD took over an hour to achieve the same. It is evident, then, that the smoothness of progress gained by using the full gradient is not worth the time cost it takes compared to SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Apply the model to your own text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier import tokenize\n",
    "from classifier import feature_extractor, classifier_agent\n",
    "import numpy as np\n",
    "\n",
    "# First load the classifier\n",
    "\n",
    "with open('data/vocab.txt') as file:\n",
    "    reading = file.readlines()\n",
    "    vocab_list = [item.strip() for item in reading]\n",
    "\n",
    "    \n",
    "# By default this is doing the bag of words, change this into your custom feature extractor\n",
    "# so it works with your \"best_model.npy\"\n",
    "feat_map = feature_extractor(vocab_list, tokenize)\n",
    "\n",
    "d = len(vocab_list)\n",
    "params = np.array([0.0 for i in range(d)])\n",
    "custom_classifier = classifier_agent(feat_map, params)\n",
    "custom_classifier.load_params_from_file('trained_params_sgd.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.] [0.]\n"
     ]
    }
   ],
   "source": [
    "# Try it out!\n",
    "\n",
    "my_sentence = \"This movie is amazing! Truly a masterpiece.\"\n",
    "\n",
    "my_sentence2 = \"The book is really, really good. The movie is just dreadful.\"\n",
    "\n",
    "ypred = custom_classifier.predict(my_sentence,RAW_TEXT=True)\n",
    "\n",
    "ypred2 = custom_classifier.predict(my_sentence2,RAW_TEXT=True)\n",
    "\n",
    "print(ypred,ypred2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also try predicting for each word in the input so as to get a sense of how the classifier arrived at the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1j/r963mnqs4wsdzdf0mj1qyvjm0000gn/T/ipykernel_34064/1404405345.py:23: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  df.style.applymap(color_predictions)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3fe19_row0_col0, #T_3fe19_row0_col1, #T_3fe19_row0_col2, #T_3fe19_row0_col3, #T_3fe19_row0_col4, #T_3fe19_row0_col5, #T_3fe19_row0_col6, #T_3fe19_row0_col7, #T_3fe19_row0_col8, #T_3fe19_row0_col9, #T_3fe19_row0_col10, #T_3fe19_row1_col0, #T_3fe19_row1_col2, #T_3fe19_row1_col3, #T_3fe19_row1_col4, #T_3fe19_row1_col6, #T_3fe19_row1_col8 {\n",
       "  color: black;\n",
       "}\n",
       "#T_3fe19_row1_col1, #T_3fe19_row1_col9, #T_3fe19_row1_col10 {\n",
       "  color: red;\n",
       "}\n",
       "#T_3fe19_row1_col5, #T_3fe19_row1_col7 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3fe19\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3fe19_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_3fe19_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_3fe19_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_3fe19_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_3fe19_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_3fe19_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
       "      <th id=\"T_3fe19_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
       "      <th id=\"T_3fe19_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n",
       "      <th id=\"T_3fe19_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n",
       "      <th id=\"T_3fe19_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n",
       "      <th id=\"T_3fe19_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe19_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3fe19_row0_col0\" class=\"data row0 col0\" >the</td>\n",
       "      <td id=\"T_3fe19_row0_col1\" class=\"data row0 col1\" >book</td>\n",
       "      <td id=\"T_3fe19_row0_col2\" class=\"data row0 col2\" >is</td>\n",
       "      <td id=\"T_3fe19_row0_col3\" class=\"data row0 col3\" >really</td>\n",
       "      <td id=\"T_3fe19_row0_col4\" class=\"data row0 col4\" >really</td>\n",
       "      <td id=\"T_3fe19_row0_col5\" class=\"data row0 col5\" >good</td>\n",
       "      <td id=\"T_3fe19_row0_col6\" class=\"data row0 col6\" >the</td>\n",
       "      <td id=\"T_3fe19_row0_col7\" class=\"data row0 col7\" >movie</td>\n",
       "      <td id=\"T_3fe19_row0_col8\" class=\"data row0 col8\" >is</td>\n",
       "      <td id=\"T_3fe19_row0_col9\" class=\"data row0 col9\" >just</td>\n",
       "      <td id=\"T_3fe19_row0_col10\" class=\"data row0 col10\" >dreadful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe19_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3fe19_row1_col0\" class=\"data row1 col0\" >-0.009220</td>\n",
       "      <td id=\"T_3fe19_row1_col1\" class=\"data row1 col1\" >-0.192747</td>\n",
       "      <td id=\"T_3fe19_row1_col2\" class=\"data row1 col2\" >0.007395</td>\n",
       "      <td id=\"T_3fe19_row1_col3\" class=\"data row1 col3\" >0.014397</td>\n",
       "      <td id=\"T_3fe19_row1_col4\" class=\"data row1 col4\" >0.014397</td>\n",
       "      <td id=\"T_3fe19_row1_col5\" class=\"data row1 col5\" >0.137312</td>\n",
       "      <td id=\"T_3fe19_row1_col6\" class=\"data row1 col6\" >-0.009220</td>\n",
       "      <td id=\"T_3fe19_row1_col7\" class=\"data row1 col7\" >0.038255</td>\n",
       "      <td id=\"T_3fe19_row1_col8\" class=\"data row1 col8\" >0.007395</td>\n",
       "      <td id=\"T_3fe19_row1_col9\" class=\"data row1 col9\" >-0.117905</td>\n",
       "      <td id=\"T_3fe19_row1_col10\" class=\"data row1 col10\" >-0.145225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1212aef30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# function for set text color of positive\n",
    "# values in Dataframes\n",
    "def color_predictions(val):\n",
    "    eps = 0.02\n",
    "    if isinstance(val,float):\n",
    "        if val > eps:\n",
    "            color = 'blue'\n",
    "        elif val < -eps:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'black'\n",
    "    else:\n",
    "        color='black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "my_sentence_list = tokenize(my_sentence2)\n",
    "ypred_per_word = custom_classifier.predict(my_sentence_list,RAW_TEXT=True,RETURN_SCORE=True)\n",
    "\n",
    "df = pd.DataFrame([my_sentence_list,ypred_per_word])\n",
    "\n",
    "df.style.applymap(color_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the questions: \n",
    "1. Are the above results making intuitive sense and why?\n",
    "    - Yes, the results make intuitive sens. For the words that are just preopsitions are non-adjectives, they are given close to zero weight as both types of reviews will have these and they do not inform the agent on what kind of review this is. The words \"good\" and \"movie\" are both positively weighted and that makes sense. Being a positive adjective, \"good\" will, in most cases, imply that this review is saying something positive. On the other hand, the word \"movie\" is closer to a neutal word, which is why it is weighted less than the word \"good\". However, it is still slightly positive as it mentions the movie itself. Mirroring this, the words \"book\", \"just\", and \"dreadful\" all have negative weights. Ngative adjectives aside, the word \"book\" has a negative weight because it is talking about the foil of the movie and in many cases, people mention the book as a way to compare the movie to.\n",
    "2. What are some limitation of a linear classifier with BoW features?\n",
    "    - Each word is given a weight. However, this also means that each word is treated as an independent feature. This means that the classifier will not be able to take into account the context of the words. For example, the word \"good\" is given a positive weight. However, if the word \"not\" is placed before it, the classifier will not be able to take into account that the word \"not\" negates the word \"good\".\n",
    "3. what are some ideas you can come up with to overcome these limitations (i.e., what are your ideas of constructing informative features)?\n",
    "    - One way to overcome this limitation is to use n-grams. This way, the classifier will be able to take into account the context of the words. For example, the word \"not\" will be able to negate the word \"good\" if they are both in the same n-gram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Document what you did for custom feature extractors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you try? What were the accuracy you got. What worked better and what not, and why?\n",
    "\n",
    "Please provide a detailed documentation to what you did. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5:  Anything else you'd like to write about. Your instructor / TA will read them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I relearned that Python3 type hints are not as useful as I want them to. The toughest part was to keep track of the data types that I am working with and to try and not convert things into dense matricies when I don't need to. Otherwise, the implementation of the lab was intuitive enough from the material learned in lecture. I wouldn't say it was easy as there was a moment when everything had to click and be translated from theory in lecture into practice in the lab. The part that I worked on the longest was translating the abstract loss equation into concrete code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
